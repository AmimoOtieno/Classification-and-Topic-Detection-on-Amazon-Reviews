# -*- coding: utf-8 -*-
"""IDTA Coursework 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wb_V3hFWRbEu8pnZLLDaV1wW03NYCTPb

**Imports**
"""

import numpy as np
import pandas as pd
import nltk #natural language toolkit
import string #provides useful constants and functions related to string handling

from nltk.tokenize import word_tokenize #used to split text into individual words (tokens)
from nltk.corpus import stopwords #This imports the stopwords corpus from NLTK.
from sklearn.feature_extraction.text import CountVectorizer #used to convert a collection of text documents into a matrix of token counts
from sklearn.feature_extraction.text import TfidfVectorizer # it represents text in terms of Term Frequency-Inverse Document Frequency (TF-IDF)

nltk.download('stopwords')
nltk.download('wordnet')

# import the csv file
reviews = pd.read_csv("amazon_cells_labelled.csv")

#check data
reviews.head()

#check the number of rows and columns
reviews.shape

"""**Label Encoding**"""

# Calculate value counts for sentiments
reviews['Sentiment'].value_counts()

#checking the datatypes
reviews.dtypes

#save the labels and encode them as 1 and 0 for future classification/clustering
from sklearn.preprocessing import LabelEncoder

enc = LabelEncoder()
label = enc.fit_transform(reviews['Sentiment'])
print(label[:10])
print(reviews['Sentiment'][:10])

#changing the text column datatype to string
reviews = reviews.astype({'Text':'string'})

#checking the datatypes
reviews.dtypes

#extract the review text for preprocessing
text = reviews['Text']
text[:10]

"""**TASK 1: TEXTUAL DATA PREPROCESSING**

**i. Removing Special Characters**
"""

import re

# Define a function to remove specific special characters
def remove_special_characters(text):
    # Replace $, %, *, and # with an empty string
    return re.sub(r'[\$\%\*\#]', '', text)

# Create an empty list to store the cleaned data
text1 = []

# Process each line in the text
for line in text:
    # Clean each line and add it to the list
    cleaned_line = remove_special_characters(line)
    text1.append(cleaned_line)

text1[:5]

text[156]

text1[156]

text[811]

text1[811]

text[453]

text1[453]

text[232]

text1[232]

text[726]

text1[726]

"""**ii. Removing** **Punctuation**"""

# Creating an empty list to store the cleaned versions of the reviews
text2 = []

import string

# Iterate through each review in the text list and remove punctuation
for review in text1:
    review = review.translate(str.maketrans('', '', string.punctuation))
    text2.append(review)

# Check the first 10 reviews after removing punctuation
text2[:10]

text1[1]

text2[1]

text1[3]

text2[3]

text1[7]

text2[7]

text1[8]

text2[8]

text1[25]

text2[25]

"""**iii. Removing Numbers**"""

# Create a new list to store reviews with digits removed
text3 = []

# Iterate through the reviews in text1 and remove digits
for review in text2:
    review = review.translate(str.maketrans('', '', string.digits))
    text3.append(review)

# Check the first 5 reviews after removing digits
text3[:5]

text2[3]

text3[3]

text2[12]

text3[12]

text2[26]

text3[26]

text2[107]

text3[107]

text2[108]

text3[108]

"""**iii. Converting to lower case**"""

# Create a new list to store reviews in lowercase
text4 = []

# Iterate through the reviews in text2 and convert to lowercase
for review in text3:
    review = review.lower()
    text4.append(review)

# Check the first 5 reviews after converting to lowercase
text4[:5]

text3[0]

text4[0]

text3[1]

text4[1]

text3[2]

text4[2]

text3[956]

text4[956]

text3[976]

text4[976]

text4 = pd.Series(text4)
text4[:5]

"""**iv. Stop** **Words**"""

#remove stop words

#Setting English stopwords
stop_words = set(stopwords.words('english'))

text5 = text4.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))
text5[:5]

text4[10]

text5[10]

text4[20]

text5[20]

text4[30]

text5[30]

text4[40]

text5[40]

text4[50]

text5[50]

"""**v. Lemmatising**"""

from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')

#apply lemmatising with POS tags

def get_wordnet_pos(word):
    #Map POS tag to first character lemmatize() accepts
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

# Init the Wordnet Lemmatizer
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('averaged_perceptron_tagger_eng')
lemmatizer = WordNetLemmatizer()
text6 = text5.apply(lambda x:' '.join(lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in x.split()))
text6[:5]

text5[100]

text6[100]

text5[120]

text6[120]

text5[150]

text6[150]

text5[210]

text6[210]

text5[220]

text6[220]

#This code is focused on creating a pandas DataFrame from the preprocessed text and sentiment labels
reviews1 = list(zip(text6, label))

reviewsP = pd.DataFrame (reviews1, columns = ['Review', 'Sentiment'])
reviewsP

"""**TASK 2: CLASSIFICATION**"""

reviewsP.head()

# Shuffling the data to prevent any ordering to have an effect on the performance
reviewsP1 = reviewsP.sample(frac=1, random_state=1).reset_index()
reviewsP1.head()

#splitting the dataset

#train dataset by splitting the data
train_reviews = reviewsP1.Review[:800]
train_sentiments = reviewsP1.Sentiment[:800]

#test dataset
test_reviews = reviewsP1.Review[800:]
test_sentiments = reviewsP1.Sentiment[800:]

print(train_reviews.shape,train_sentiments.shape)
print(test_reviews.shape,test_sentiments.shape)

train_reviews[0]

# Check class distribution in training data
print(train_sentiments.value_counts())

"""*Bag of words*"""

#Count vectorizer for bag of words
#cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))
cv=CountVectorizer()

#transformed train reviews
cv_train_reviews=cv.fit_transform(train_reviews)
#transformed test reviews
cv_test_reviews=cv.transform(test_reviews)

print('BOW_cv_train:',cv_train_reviews.shape)
print('BOW_cv_test:',cv_test_reviews.shape)

vocab=cv.get_feature_names_out() #toget feature names

print(cv_train_reviews[0])

#Oversample the training data
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(cv_train_reviews, train_sentiments)

# Check class distribution in training data
print(y_train_resampled.value_counts())

"""**Decision Tree**"""

from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score # Importing necessary functions
import numpy as np

# Initialize the DecisionTreeClassifier
DT = DecisionTreeClassifier(criterion='entropy', random_state=0)

# Cross-validation for Bag of Words
cv_bow_scores = cross_val_score(DT, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
print("DT_bow accuracy:", np.mean(cv_bow_scores))

# Get predictions for Bag of Words using cross-validation
DT_bow_predictions = cross_val_predict(DT, X_train_resampled, y_train_resampled, cv=5)

# Confusion Matrix and Classification Report for Bag of Words
print("\nConfusion Matrix for Bag of Words:")
print(confusion_matrix(y_train_resampled, DT_bow_predictions))
print("\nClassification Report for Bag of Words:")
print(classification_report(y_train_resampled, DT_bow_predictions, target_names=['Negative', 'Positive']))

# ROC AUC for Bag of Words
roc_auc_bow = roc_auc_score(y_train_resampled, DT_bow_predictions)
print(f"\nROC AUC for Bag of Words: {roc_auc_bow:.4f}")

"""**k-NN**"""

from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
import matplotlib.pyplot as plt

# Initialize the KNeighborsClassifier
kNN = KNeighborsClassifier(n_neighbors=7)

# Cross-validation for Bag of Words
cv_bow_scores = cross_val_score(kNN, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
print("kNN_bow accuracy:", np.mean(cv_bow_scores))

# Get predictions from cross-validation for Bag of Words
kNN_bow_predictions = cross_val_predict(kNN, X_train_resampled, y_train_resampled, cv=5)

# Confusion Matrix and Classification Report for Bag of Words
print("\nConfusion Matrix for Bag of Words:")
print(confusion_matrix(y_train_resampled, kNN_bow_predictions))
print("\nClassification Report for Bag of Words:")
print(classification_report(y_train_resampled, kNN_bow_predictions, target_names=['Negative', 'Positive']))

# Get probability predictions for Bag of Words
kNN_bow_probabilities = cross_val_predict(kNN, X_train_resampled, y_train_resampled, cv=5, method='predict_proba')

# Calculate ROC AUC score
roc_auc_bow = roc_auc_score(y_train_resampled, kNN_bow_probabilities[:, 1])  # Use probabilities for the positive class
print("\nkNN_bow ROC AUC:", roc_auc_bow)

"""**Naive Bayes**"""

from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import numpy as np

# Initialize the Multinomial Naive Bayes classifier
NB = MultinomialNB()

# Cross-validation for Bag of Words (Accuracy)
cv_bow_scores = cross_val_score(NB, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
print("NB_bow accuracy:", np.mean(cv_bow_scores))

# Get predictions from cross-validation for Bag of Words
NB_bow_predictions = cross_val_predict(NB, X_train_resampled, y_train_resampled, cv=5)

# Confusion Matrix and Classification Report for Bag of Words
print("\nConfusion Matrix for Bag of Words:")
print(confusion_matrix(y_train_resampled, NB_bow_predictions))
print("\nClassification Report for Bag of Words:")
print(classification_report(y_train_resampled, NB_bow_predictions, target_names=['Negative', 'Positive']))

# Get probability predictions for Bag of Words
NB_bow_probabilities = cross_val_predict(NB, X_train_resampled, y_train_resampled, cv=5, method='predict_proba')

# Calculate ROC AUC score
roc_auc_bow = roc_auc_score(y_train_resampled, NB_bow_probabilities[:, 1])  # Use probabilities for the positive class
print("\nNB_bow ROC AUC:", roc_auc_bow)

"""**Random Forest**"""

from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# Initialize the Random Forest Classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Cross-validation for Bag of Words (Accuracy)
cv_bow_scores = cross_val_score(rf_clf, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')
print("Random Forest_bow accuracy:", np.mean(cv_bow_scores))

# Get predicted probabilities from cross-validation for Bag of Words
rf_bow_probabilities = cross_val_predict(rf_clf, X_train_resampled, y_train_resampled, cv=5, method='predict_proba')

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_train_resampled, rf_bow_probabilities[:, 1])  # Get probabilities for the positive class
print("\nROC AUC Score for Bag of Words:", roc_auc)

# Get predictions from cross-validation for Bag of Words
rf_bow_predictions = cross_val_predict(rf_clf, X_train_resampled, y_train_resampled, cv=5)

# Confusion Matrix and Classification Report for Bag of Words
print("\nConfusion Matrix for Bag of Words:")
print(confusion_matrix(y_train_resampled, rf_bow_predictions))
print("\nClassification Report for Bag of Words:")
print(classification_report(y_train_resampled, rf_bow_predictions, target_names=['Negative', 'Positive']))

"""**TASK 3: BERT CLASSIFICATION**"""

# A dependency of the preprocessing for BERT inputs
!pip3 install tensorflow-text

#to solve dependency issue for tf-models-official
!pip install numba

!pip install "tf-models-official==2.13.*"

!pip install transformers

import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

import pandas as pd

reviews = pd.read_csv("amazon_cells_labelled.csv")

reviews.head()

reviews.shape

reviews['Sentiment'].value_counts()

#save the labels and encode them as 1 and 0 for future classification/clustering
from sklearn.preprocessing import LabelEncoder

enc = LabelEncoder()
label = enc.fit_transform(reviews['Sentiment'])
print(label[:10])
print(reviews['Sentiment'][:10])

reviews1 = list(zip(reviews['Text'], label))

reviews1 = pd.DataFrame (reviews1, columns = ['Review', 'Sentiment'])
reviews1

reviews1 = reviews1.sample(frac=1, random_state=1)
reviews1.reset_index(drop=True, inplace=True)

reviews1.head()

from sklearn.model_selection import train_test_split

#split and take the test set
X, X_test, y, y_test = (train_test_split(reviews1['Review'], reviews1['Sentiment'],
                                                     stratify=reviews1['Sentiment'],
                                                     test_size=0.2,
                                                     train_size=0.8
                                                     ))

#split the train set (X) into train and validation
X_train, X_val, y_train, y_val = (train_test_split(X, y,
                                                     stratify=y,
                                                     test_size=0.25,
                                                     train_size=0.75
                                                     ))

X_train

X_val

X_test

#@title Choose a BERT model to fine-tune

#the model has 5 layers (L), 512 hidden size H and 8 attention heads
bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'

map_name_to_handle = {
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
}

map_model_to_preprocess = {
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
}

tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')

bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)

reviews1['Review'][0]

text_test = ["The holster that arrived did not match the photo in the ad"]
text_preprocessed = bert_preprocess_model(text_test)

print(f'Keys       : {list(text_preprocessed.keys())}')
print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')

bert_model = hub.KerasLayer(tfhub_handle_encoder)

bert_results = bert_model(text_preprocessed)

print(f'Loaded BERT: {tfhub_handle_encoder}')
print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  net = tf.keras.layers.Dropout(0.1)(net)
  #net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)
  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)

classifier_model = build_classifier_model()

tf.keras.utils.plot_model(classifier_model)

epochs = 6
steps_per_epoch = 1200  #corresponding to the train set size
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps) #10% of num_train_steps

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

#fine-tune BERT for classification
classifier_model.compile(optimizer,
                     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                     metrics=[tf.keras.metrics.BinaryAccuracy()])
classifier_model.summary()

tf.keras.utils.plot_model(model=classifier_model,show_shapes=True,dpi=76)

print(f'Training model with {tfhub_handle_encoder}')
history = classifier_model.fit(X_train,
                               y_train,
                               validation_data=(X_val, y_val),
                               epochs=epochs)

history_dict = history.history
print(history_dict.keys())

acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()

plt.subplot(2, 1, 1)
# r is for "solid red line"
plt.plot(epochs, loss, 'r', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

loss, accuracy = classifier_model.evaluate(X_test, y_test)

print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

#performance metrics
from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

#predicted values using the model
y_pred=classifier_model.predict(X_test)

print(y_pred.flatten())

#len(y_pred)

for i in range(len(y_pred)):
  if y_pred[i]<0.5:
    y_pred[i] = 0
  else: y_pred[i] = 1

print(y_pred.flatten())

import numpy as np
from sklearn.metrics import roc_auc_score

print(confusion_matrix(y_test, y_pred), '\n')
#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
#print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)), '\n')

print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("AUC:", roc_auc_score(y_test, y_pred))

"""**TASK 4: TOPIC DETECTION**

**1. LDA**
"""

data = reviewsP.Review.values.tolist()

data

!pip install gensim
import gensim # Add this line to import the gensim library

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence)))

words = list(sent_to_words(data))

print(words[:1][0][:30])

import gensim.corpora as corpora

# Create Dictionary
id2word = corpora.Dictionary(words)

# Create Corpus
texts = words

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

# View
print(corpus[:1][0][:30])

from pprint import pprint

# number of topics
num_topics = 10

# Build LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)

# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
#doc_lda = lda_model[corpus]

pip install pyldavis

import pyLDAvis.gensim_models
import pyLDAvis

# Visualize the topics
pyLDAvis.enable_notebook()

LDAvis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)

LDAvis

"""**2. NMF**"""

# Importing necessary libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import pandas as pd

# import the csv file
reviews = pd.read_csv("amazon_cells_labelled.csv")

reviews.head()

import string
text = reviews['Text']

text1 = []

for review in text:
    #print(sentence)
    #remove punctuation
    review = review.translate(str.maketrans('', '', string.punctuation))
    # remove digits/numbers
    review = review.translate(str.maketrans('', '', string.digits))
    #change to lowercase
    review = review.lower()
    #print(sentence)
    text1.append(review)


text1[:2]

# Step 2: Convert text to TF-IDF features
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(
    max_df=0.95,  # Ignore terms with very high document frequency
    min_df=2,     # Ignore terms with very low document frequency
    stop_words='english'  # Remove common English stopwords
)

# Fit and transform the preprocessed text
tfidf_matrix = tfidf_vectorizer.fit_transform(text1)

# Check the shape of the resulting matrix
print("TF-IDF matrix shape:", tfidf_matrix.shape)

from sklearn.decomposition import NMF

# Number of topics you want to extract
num_topics = 10

# Initialize the NMF model
nmf_model = NMF(n_components=num_topics, random_state=42)

# Fit the NMF model to the TF-IDF matrix
nmf_model.fit(tfidf_matrix)

# Extract the topic-word matrix
topic_word_matrix = nmf_model.components_

# Extract the document-topic matrix
doc_topic_matrix = nmf_model.transform(tfidf_matrix)

# Display the shape of the matrices
print("Topic-word matrix shape:", topic_word_matrix.shape)  # (num_topics, n_features)
print("Document-topic matrix shape:", doc_topic_matrix.shape)  # (n_samples, num_topics)

# Get the feature names (words) from the TF-IDF vectorizer
feature_names = tfidf_vectorizer.get_feature_names_out()

# Number of top words to display for each topic
num_top_words = 30

# Display the topics
print("\nTopics and their top words:")
for topic_idx, topic in enumerate(topic_word_matrix):
    top_words = [feature_names[i] for i in topic.argsort()[-num_top_words:][::-1]]
    print(f"Topic #{topic_idx + 1}: {', '.join(top_words)}")

